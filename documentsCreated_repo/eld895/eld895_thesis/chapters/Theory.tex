\section[Theory]{Theory}
\label{sec:theory}

\textbf{Detrended Fluctuation Analysis}: A method of analyzing real-world time series for self-affinity, i.e. how correlated a signal's future value is to it's past values. Say, $x(t)$ is a signal from a natural process, then in order to detrend it, it can first be passed via a low-pass filter $\verb|LPF|$ to obtain \verb|LPF|$(x(t))$ and the resultant signal be subtracted from the original in order to obtain the detrended version of the natural process signal $\tilde{x}(t)$ or $d(x(t))$:
\begin{equation}
	\tilde{x}(t) \verb| or | d(x(t)) = x(t) - \verb|LPF|(x(t))
\end{equation}

\textbf{Autocorrelation function}: A statistical measure of the correlation of a state variable with a time-lagged version of itself. 

\begin{equation}
	c(x(t), \tau) = \int_{-\infty}^{\infty}x(t)x(t+\tau)dt  
\end{equation}

In terms of discrete time functions, autocorrelation may be expressed as:
\begin{equation}
	c(x[n], \tau) =  \sum_{-\infty}^{\infty} x[n]x[n+\tau]
\end{equation}

It may be noted that the autocorrelation function of any time-varying variable has two degrees of freedom, the first being continuous time $t$ (or instance $n$ for discrete time functions) and the second parameter being the lag $\tau$, i.e. the time duration by which the time-varying variable is displaced/lagged against itself for performing the autocorrelation.

Since in this thesis the function is used in two ways where only one of the parameters is varying (with the other being kept constant) each time, two variations of the autocorrelation function with individual names are specified here:

\textbf{Fixed Time Autocorrelation}: Autocorrelation function computed over a snapshot of a time-varying variable over a fixed time window. The usage of Fixed Time Autocorrelation can be seen in the Offline Analysis chapter (Chapter \ref{sec:offline}) of this thesis.

\begin{equation}
	c(\tau) = \frac{\sum_{1}^{W_{total}-1} x[n]x[n+\tau]}{\sum_{1}^{W_{total}-1} x[n]^2} 
\end{equation}
 
In this thesis, the window length $W_{total}$ mostly ranges from months to years, with the exception of a few grids, where it can be for just a few days.
  
Fixed Time Autocorrelation $c(\tau)$ of any detrended physical/natural signal $\tilde{x}(t)$ or $d(x(t))$, should decrease exponentially as the time-lag $\tau$ is increased \cite{schafer01}. The decrement of Fixed Time Autocorrelation with respect to the lag $\tau$ is intuitive as it makes sense for a state variable's value to be more correlated with its value a few earlier or later ago than its value thirty minutes earlier or later.
 
 \textbf{Fixed Lag Autocorrelation}: Autocorrelation function computed over a window running over a continuously generated stream of data in which the value of the lag $\tau$ is fixed. The usage of Fixed Lag Autocorrelation can be seen in the Online Analysis chapter (Chapter \ref{sec:online}) of this thesis.
 
 \begin{equation}
 	c(t)|_{\tau = \tau_{fixed}} = \frac{\sum_{i}^{i+W-1} x[n]x[n+\tau_{fixed}]}{\sum_{i}^{i+W-1} x[n]^2} 
 \end{equation}
 
 In this thesis, the window length $W$ represents the number of consecutive discrete-time instances equivalent to $T=15$ seconds. The value of $\tau$ used is the number of consecutive discrete-time instances equivalent to $\tau_{fixed}=1$ second. More details in Chapter \ref{sec:online} of this thesis.

\textbf{Variance}: Degree of overall deviation/fluctuation in the values of a data.

\begin{equation}
	\sigma^2(x) = E(x^2) - (E(x))^2 
\end{equation}
\hspace{75pt} where,
\begin{equation}
	E(x) = \frac{1}{W} \sum_{i}^{i+W-1} x[n]
\end{equation}

\textbf{Bifurcation Theory}: The concepts Bifurcations and Critical Bifurcations were used to explain why a small yet steady change in the parameters of a dynamical system (such as the power grid) can remain inconspicuous only to, upon reaching a `tipping' point or `Critical Transition', manifest as a sudden major upset to the `motion' of the dynamical system. The terms Bifurcations and Critical Bifurcations were used almost interchangeably, although technically only Critical Bifurcations are significant enough to alter the dynamics of a system from stable to unstable. Any dynamical system can be expressed as a set of differential algebraic equations. The  Figures \ref{fig:bifPitchforkSupercritical} and \ref{fig:bifPitchforkSubcritical} demonstrate this phenomena in a particular variation, the Pitchfork bifurcation.


\begin{figure}[!ht]
	\centering{
	\import{../figures/}{bifurcationPitchforkPdf.pdf_tex}
	}
	\caption{Bifurcation diagram for the normal form of the Supercritical Pitchfork Bifurcation $\frac{dx}{dt} = \mu x - x^3$. \\ For $\mu \leq 0$, the only stable equilibrium solution for the dynamical system (or fixed point) is $x=0$. Upon reaching the critical `tipping' point $\mu=0$, $x=0$ no longer remains a stable equilibrium path for the dynamical system and instead two different stable paths emerge: $x = \sqrt{+\mu}$ and $x = -\sqrt{\mu}$. Such a critical point, in which one stable path bifurcates into at least two distinct paths (which may or may not be stable) is called a bifurcation point and the phenomenon is known as `Critical Bifurcation'.}
	\label{fig:bifPitchforkSupercritical}
\end{figure}

\begin{figure}[!ht]
	\centering{
		\import{../figures/}{bifurcationPitchforkNegativePdf.pdf_tex}
	}
	\caption{Bifurcation diagram for the normal form of the Subcritical Pitchfork Bifurcation $\frac{dx}{dt} = \mu x + x^3$. \\ For $\mu \leq 0$, the only stable equilibrium solution (or fixed point) for the dynamical system is $x=0$. Upon reaching the critical `tipping' point $\mu=0$, three different equilibrium solutions (or fixed points) emerge: $x=0$, $x = \sqrt{+\mu}$ and $x = -\sqrt{\mu}$, none of which are stable. Here too, $\mu=0$ is a bifurcation point.}
	\label{fig:bifPitchforkSubcritical}
\end{figure}


\textbf{Critical Slowing Down}: The theory of Critical Slowing Down applies on dynamical systems on the verge of `tipping' or `bifurcation' and how they show warning signs before breaking down or descending into instability. These warning signs such as an `increased time to settle', `increased autocorrelation and variance of fluctuations', etc. \cite{schefferEarlyWarningSignalsForCriticalTransitions} can be statistically analyzed to predict the onset of such a bifurcation for a given system.

\noindent \textbf{Why do Autocorrelation and Variance increase with Critical Slowing Down?}:

The question may be divided into two parts:
\begin{enumerate}
	\item How does the autocorrelation of the perturbations of a system's state variables increase when the system is undergoing Critical Slowing Down?
	\item For a system with increasing autocorrelation of the perturbations its state variables, how does the variance of the perturbations also increase?
\end{enumerate}

As an answer to the first question, we may consider the example given in \cite{schefferEarlyWarningSignalsForCriticalTransitions}:

Let a dynamical system be represented by the differential function
\begin{equation}
	\label{eq:exampleDynamicalSystem}
	\frac{dx}{dt} = k(x-x_1)(x-x_2)
\end{equation}
\hspace{25pt} where $k$ is a positive coefficient, $x_1$ and $x_2$ are parameters, possibly as result of inherent dynamics of the system infrastructure or an intentional control mechanism (in which case, they are also known as control parameters).

Let $x_1$ be the stable equilibrium point (or stable fixed point) and $x_2$ be the unstable equilibrium point (unstable fixed point	). This implies that $x_1 < x_2$ (one may easily prove this by taking the second derivatives of Equation \ref{eq:exampleDynamicalSystem} and checking for its sign at the fixed points).

Now if the system were to be perturbed at its stable fixed point $x_1$ by a small disturbance of $\Delta x$, the perturbed dynamical system could be expressed by a slight variation of Equation \ref{eq:exampleDynamicalSystem}:

\begin{equation}
	\frac{d(x_1+\Delta x)}{dt} = k(x_1+\Delta x-x_1)(x_1+\Delta x - x_2)
\end{equation}
\hspace{25pt} or,

\begin{equation}
	\frac{d(x_1)}{dt} + \frac{d(\Delta x)}{dt} = k(\Delta x)(\Delta x + x_1 - x_2)
\end{equation}
\hspace{25pt} or,

\NewDocumentCommand{\evalat}{sO{\big}mm}{%
	\IfBooleanTF{#1}
	{\mleft. #3 \mright|_{#4}}
	{#3#2|_{#4}}%
}
\begin{equation}
	\evalat[\bigg]{\frac{d(\Delta x)}{dt}}{x_1} = k\left\{(x_1-x_2)\Delta x + (\Delta x)^2\right\}
\end{equation}
\hspace{25pt} Ignoring the second order terms:

\begin{equation}
	\evalat[\bigg]{\frac{d(\Delta x)}{dt}}{x_1} = k(x_1-x_2)\Delta x
\end{equation}
\hspace{25pt} Since the perturbation $\Delta x$ is independent of the parameters $x_1$ and $x_2$, the above equation may be rewritten by replacing $k(x_1-x_2)$ with $\lambda_1$:

\begin{equation}
	\evalat[\bigg]{\frac{d(\Delta x)}{dt}}{x_1} = \lambda_1 \Delta x 
\end{equation}
Thus the perturbation may be expressed as an explicit time domain equation:
\begin{equation}
	\Delta x(t)|_{x_1} = \exp{(\lambda_1 t)}
\end{equation}
which has a recovery rate of $\lambda_1 = k(x_1-x_2)$. On the other hand, a perturbation at the unstable fixed point $x_2$ would become exponentially increasing.
\begin{equation}
	\evalat[\bigg]{\frac{d(\Delta x)}{dt}}{x_2} = \exp{(\lambda_2 t)}
\end{equation}
 \hspace{25pt} where $\lambda_2 = -\lambda_1 = -k(x_1-x_2)$
 
It may be seen from the above two equations representing the perturbations at the two fixed points that their recovery (or blow-up) rates depend on the difference in the values of the parameters $x_1$ and $x_2$. At the special case of $x_1 = x_2$, the fixed points exchange stability and the recovery rates tend to zero (Refer to the critical point $\mu$ in figures \ref{fig:bifPitchforkSupercritical} and \ref{fig:bifPitchforkSubcritical}) for an example of the phenomena in two variations of the Pitchfork Bifurcation).

The second question is also answered in the same reference  \cite{schefferEarlyWarningSignalsForCriticalTransitions}. It explains the increase of autocorrelation and variance when a dynamical system trends towards instability with the help of the example below:

Consider a discrete-time stochastic dynamical system with the vector of state variables being depicted by $x$. Let $x_n$ depict the value of the state variable vector $x$ at discrete time instance $n$.
Now, if an additive noise say, Gaussian noise, is added to the stochastic dynamical system at every discrete time interval $\Delta t$, the slightly perturbed set of state variables, say $x_{n+1}$, of the otherwise stable dynamical system, would attempt to return to the set of state variables before the perturbation $x_n$, with an exponential recovery speed.

\begin{equation}
	x_{n+1} - x_{n} = \exp{(\lambda \Delta t)}(x_{n+1}-x_{n}) + \zeta_n\sigma
\end{equation}

\hspace{25pt} where $\sigma$ is the standard deviation representing the stochastic noise (assumed Gaussian) inherent to the stochastic dynamical system and $\zeta_n$ represents a coefficient to $\sigma$ to model the magnitude of the stochastic noise at discrete-time instant $n$. $\lambda$ represents the rate of exponential recovery.

The term $x_{n+1} - x_{n}$ may then simply be replaced with term $y_{n+1}$ which depicts the deviation of the vector of state variables $x$ between discrete-time instances $n$ and $n+1$.

\begin{equation}
	y_{n+1} = \exp{(\lambda \Delta t)}y_n + \zeta_n
	\sigma
\end{equation}

Now if the time intervals between perturbations $\Delta t$ and the recovery rate $\lambda$ are independent of the state variables $x_{n}$ (and therefore their perturbations $y_{n}$), the above equation may be rewritten in the form of an autoregression model of the first order:

\begin{equation}
	\label{eq:autocorrNoConstant}
	y_{n+1} = \alpha y_{n} + \zeta_n\sigma
\end{equation}

The autocorrelation constant $\alpha \equiv \exp{(\lambda \Delta t)}$ is zero for a purely white gaussian noise and can be one or even greater for other kinds of highly correlated noises in stochastic processes such as brown noise (un-normalized data only).

For the general equation of a lag-1 autoregression:
\begin{equation}
	\label{eq:autocorrGeneral}
	y_{n+1} = \alpha_0 + \alpha y_{n} + \zeta_n\sigma
\end{equation} 
the expected value (mean) of $y_{n}$ may be computed using:
\begin{equation}
	\EX(y_{n+1}) = \EX(\alpha_0) + \EX(\alpha y_{n}) + \EX(\zeta_n \sigma)
\end{equation}
\hspace{25pt} or,
\begin{equation}
	\mu = \alpha_0 + \alpha\mu + 0
\end{equation}
\hspace{25pt} or
\begin{equation}
	\label{eq:meanFormulaGeneral}
	\mu = \frac{\alpha_0}{1-\alpha}
\end{equation}

Thus, if $\alpha_0 = 0$, as in Equation \ref{eq:autocorrNoConstant}, the mean of autoregression $\mu$ will also be zero.

The variance of $y_{n}$, $\sigma^2_{y_{n}}$ may be computed using the formula for variance:

\begin{equation}
	\sigma^2_{y_{n}} = \EX(y^2_{n}) - \left(\EX(y_{n})\right)^2
\end{equation} 	
\hspace{25pt} or,
\begin{equation}
	\label{eq:sigmaComputationBegins}
	\sigma^2_{y_{n}} = \EX(y^2_{n}) - \mu^2
\end{equation}

For computing the first term of the RHS of Equation \ref{eq:sigmaComputationBegins}, Equation \ref{eq:autocorrGeneral} may once again be utilized by taking the squares on both of its sides:

\begin{equation}
	y^2_{n+1} = \alpha^2_0 + \alpha^2 y^2_{n} + \zeta^2_n \sigma^2 + 2\alpha_0 \alpha y_{n} + 2\alpha_0 \zeta_n \sigma + 2\alpha y_{n} \zeta_n \sigma
\end{equation}
\hspace{25pt} or,
\begin{equation}
	\EX(y^2_{n}) = \alpha^2_0 + \alpha^2 \EX(y^2_{n}) + \zeta^2_{n} \sigma^2 + 2\alpha_0\alpha\EX(y_{n}) + 0 + 0
\end{equation}
\hspace{25pt} or,
\begin{equation}
	\EX(y^2_{n}) = \frac{\alpha^2_{0} + 2\alpha_0\alpha\mu + \zeta^2_{n}\sigma^2}{1-\alpha^2}
\end{equation}
Substituting the value of $\mu$ as per Equation \ref{eq:meanFormulaGeneral}:
\begin{equation}
	\label{eq:computedExpectationOfSquaresOfPerturbations}
	\EX(y^2_{n}) = \frac{\alpha^2_{0} + 2\alpha_0\alpha\frac{\alpha_0}{1-\alpha} + \zeta^2_{n}\sigma^2}{1-\alpha^2}
\end{equation}

 The LHS of Equation \ref{eq:computedExpectationOfSquaresOfPerturbations} and \ref{eq:meanFormulaGeneral} may now be substituted in Equation \ref{eq:sigmaComputationBegins} in order to get:
 
 \begin{equation}
 	\sigma^2_{y_{n}} = \frac{\alpha^2_{0} + 2\alpha_0\alpha\frac{\alpha_0}{1-\alpha} + \zeta^2_{n}\sigma^2}{1-\alpha^2} - \left(\frac{\alpha_0}{1-\alpha}\right)^2
 \end{equation}

Putting $\alpha_0 = 0$ as per Equation \ref{eq:autocorrNoConstant}, the final expression for $\sigma^2_{y_n}$ comes out to be:

\begin{equation}
	\label{eq:varianceFormulaNoConstant}
	\sigma^2_{y_n} = \frac{\zeta^2_{n}\sigma^2}{1-\alpha^2}
\end{equation}

As a process (represented by a vector of state variables, such as $x$) approaches towards a critical bifurcation point, it undergoes Critical Slowing Down in which its recovery rate $\lambda$ progressively slows down to zero.  The slowing down of the recovery of the process from any perturbation causes the perturbations to become progressively correlated (i.e. $\alpha$ increases towards one) and allows unrestricted increase in variance of its perturbations $\sigma^2_{y_{n}}$ (towards infinity).

Thus the mathematical relationship between Critical Slowing Down and a corresponding increase in both Autocorrelation and Variance was established. 